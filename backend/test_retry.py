#!/usr/bin/env python3
"""
Test script for the enhanced retry functionality
"""
import asyncio
import sys
import json
from typing import Dict, Any

# Add the backend path to sys.path
sys.path.append('/home/bizon/Development/chrono-scraper-fastapi-2/backend')

from app.core.database import AsyncSessionLocal
from app.models.project import Project, Domain, ScrapeSession, ScrapeSessionStatus
from app.models.scraping import ScrapePage, ScrapePageStatus
from app.models.user import User
from app.tasks.scraping_simple import process_page_content
from app.services.websocket_service import (
    broadcast_page_progress_sync,
    broadcast_session_stats_sync
)
from sqlmodel import select
from datetime import datetime

async def create_test_scenario():
    """Create a test scenario with failed pages for retry testing"""
    async with AsyncSessionLocal() as db:
        # Get or create a test user
        user_result = await db.execute(select(User).limit(1))
        user = user_result.scalar_one_or_none()
        
        if not user:
            print("‚ùå No users found. Please create a user first.")
            return None, None, None
        
        print(f"‚úÖ Found test user: {user.email}")
        
        # Create a test project
        project = Project(
            name="Retry Test Project",
            description="Testing retry functionality with live progress updates",
            user_id=user.id
        )
        db.add(project)
        await db.commit()
        await db.refresh(project)
        print(f"‚úÖ Created test project: {project.name} (ID: {project.id})")
        
        # Create a test domain
        domain = Domain(
            project_id=project.id,
            domain_name="retry-test.com",
            status="active"
        )
        db.add(domain)
        await db.commit()
        await db.refresh(domain)
        print(f"‚úÖ Created test domain: {domain.domain_name} (ID: {domain.id})")
        
        # Create a scrape session
        session = ScrapeSession(
            project_id=project.id,
            status=ScrapeSessionStatus.RUNNING,
            started_at=datetime.utcnow()
        )
        db.add(session)
        await db.commit()
        await db.refresh(session)
        print(f"‚úÖ Created scrape session: {session.id}")
        
        # Create some test scrape pages with failed status
        test_urls = [
            "https://retry-test.com/page1",
            "https://retry-test.com/page2", 
            "https://retry-test.com/page3",
            "https://retry-test.com/page4",
            "https://retry-test.com/page5"
        ]
        
        scrape_pages = []
        for i, url in enumerate(test_urls):
            # Create some failed pages and some completed ones
            status = ScrapePageStatus.FAILED if i < 3 else ScrapePageStatus.COMPLETED
            error_msg = f"Simulated failure for {url}" if status == ScrapePageStatus.FAILED else None
            
            page = ScrapePage(
                domain_id=domain.id,
                scrape_session_id=session.id,
                original_url=url,
                wayback_url=f"https://web.archive.org/web/20240101000000/{url}",
                unix_timestamp="20240101000000",
                mime_type="text/html",
                status_code=200 if status == ScrapePageStatus.COMPLETED else 404,
                status=status,
                error_message=error_msg,
                error_type="test_error" if status == ScrapePageStatus.FAILED else None,
                retry_count=0
            )
            db.add(page)
            scrape_pages.append(page)
        
        await db.commit()
        
        # Refresh all pages to get their IDs
        for page in scrape_pages:
            await db.refresh(page)
        
        failed_pages = [p for p in scrape_pages if p.status == ScrapePageStatus.FAILED]
        completed_pages = [p for p in scrape_pages if p.status == ScrapePageStatus.COMPLETED]
        
        print(f"‚úÖ Created {len(failed_pages)} failed pages and {len(completed_pages)} completed pages")
        
        return project.id, session.id, failed_pages

async def test_individual_retry(session_id: int, failed_pages: list):
    """Test individual page retry functionality"""
    if not failed_pages:
        print("‚ùå No failed pages to test individual retry")
        return
    
    test_page = failed_pages[0]
    print(f"\nüîÑ Testing individual page retry for: {test_page.original_url}")
    print(f"   Page ID: {test_page.id}")
    print(f"   Current status: {test_page.status}")
    print(f"   Retry count: {test_page.retry_count}")
    
    # Broadcast retry started event
    broadcast_page_progress_sync({
        "scrape_session_id": session_id,
        "scrape_page_id": test_page.id,
        "domain_id": test_page.domain_id or 0,
        "domain_name": "retry-test.com",
        "page_url": test_page.original_url,
        "wayback_url": test_page.wayback_url or "",
        "status": ScrapePageStatus.PENDING,
        "processing_stage": "retry_queued",
        "stage_progress": 0.0,
        "retry_count": test_page.retry_count + 1
    })
    
    print("   ‚úÖ Broadcasted retry start event")
    
    # Simulate retry processing (this would normally be done by Celery)
    print("   üîÑ Processing retry...")
    
    async with AsyncSessionLocal() as db:
        # Update page status to pending for retry
        page_result = await db.execute(select(ScrapePage).where(ScrapePage.id == test_page.id))
        page = page_result.scalar_one_or_none()
        
        if page:
            page.status = ScrapePageStatus.PENDING
            page.retry_count += 1
            page.error_message = None
            page.error_type = None
            await db.commit()
            print(f"   ‚úÖ Updated page status to PENDING (retry count: {page.retry_count})")
    
    return test_page.id

async def test_bulk_retry(session_id: int, failed_pages: list):
    """Test bulk retry functionality"""
    if not failed_pages:
        print("‚ùå No failed pages to test bulk retry")
        return
    
    print(f"\nüîÑ Testing bulk retry for {len(failed_pages)} failed pages")
    
    async with AsyncSessionLocal() as db:
        retry_count = 0
        for page in failed_pages:
            page_result = await db.execute(select(ScrapePage).where(ScrapePage.id == page.id))
            db_page = page_result.scalar_one_or_none()
            
            if db_page and db_page.status == ScrapePageStatus.FAILED:
                db_page.status = ScrapePageStatus.PENDING
                db_page.retry_count += 1
                db_page.error_message = None
                db_page.error_type = None
                retry_count += 1
        
        await db.commit()
        print(f"   ‚úÖ Updated {retry_count} pages to PENDING for retry")
    
    # Broadcast bulk retry event
    broadcast_session_stats_sync({
        "scrape_session_id": session_id,
        "total_urls": len(failed_pages) + 2,  # Include completed pages
        "pending_urls": retry_count,
        "in_progress_urls": 0,
        "completed_urls": 2,  # Completed pages
        "failed_urls": 0,  # Reset since we're retrying
        "skipped_urls": 0,
        "progress_percentage": (2 / (len(failed_pages) + 2)) * 100,
        "active_domains": 1,
        "completed_domains": 0,
        "failed_domains": 0,
        "performance_metrics": {
            "retry_operation": True,
            "pages_queued_for_retry": retry_count
        }
    })
    
    print("   ‚úÖ Broadcasted bulk retry session stats")
    return retry_count

async def test_progress_tracking():
    """Test the complete retry functionality with progress tracking"""
    print("üöÄ Starting retry functionality test with live progress tracking...")
    print("=" * 60)
    
    # Create test scenario
    project_id, session_id, failed_pages = await create_test_scenario()
    
    if not project_id:
        print("‚ùå Failed to create test scenario")
        return
    
    print(f"\nüìä Test scenario created:")
    print(f"   Project ID: {project_id}")
    print(f"   Session ID: {session_id}")
    print(f"   Failed pages: {len(failed_pages) if failed_pages else 0}")
    
    # Test individual retry
    if failed_pages:
        test_page_id = await test_individual_retry(session_id, failed_pages)
        print(f"   Individual retry test completed for page {test_page_id}")
    
    # Test bulk retry
    if failed_pages:
        retry_count = await test_bulk_retry(session_id, failed_pages[1:])  # Skip the already retried page
        print(f"   Bulk retry test completed for {retry_count} pages")
    
    print("\nüéâ Retry functionality test completed successfully!")
    print("=" * 60)
    print("\nFeatures demonstrated:")
    print("‚úÖ Individual page retry with progress tracking")
    print("‚úÖ Bulk retry for all failed pages")
    print("‚úÖ Real-time WebSocket progress broadcasting")
    print("‚úÖ Retry count tracking")
    print("‚úÖ Status transitions (FAILED ‚Üí PENDING ‚Üí processing)")
    print("‚úÖ Error message clearing on retry")
    print("\nTo test in the UI:")
    print(f"1. Navigate to: http://localhost:5173/projects/{project_id}")
    print("2. Go to the 'Live Progress' tab")
    print("3. Click retry buttons for individual pages or 'Retry All Failed'")
    print("4. Watch real-time progress updates!")

if __name__ == "__main__":
    asyncio.run(test_progress_tracking())