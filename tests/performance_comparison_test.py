#!/usr/bin/env python3
"""
Performance comparison test: Wayback Machine vs Firecrawl
"""
import asyncio
import time
import psutil
import logging
from datetime import datetime
from pathlib import Path
import sys

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent / "backend"))

from app.services.wayback_machine import fetch_cdx_pages
from app.services.content_extractor import extract_content_from_record
from app.services.parallel_cdx_fetcher import fetch_cdx_records_parallel

# Mock Firecrawl for comparison (replace with actual Firecrawl if available)
class MockFirecrawlClient:
    """Mock Firecrawl client for comparison"""
    async def scrape_pages(self, urls, max_pages=None):
        """Simulate Firecrawl scraping with realistic delays"""
        results = []
        for i, url in enumerate(urls[:max_pages] if max_pages else urls):
            # Simulate Firecrawl processing time (more realistic)
            await asyncio.sleep(0.5)  # 0.5s per page (typical Firecrawl speed)
            results.append({
                'url': url,
                'title': f'Mock Title {i}',
                'content': f'Mock content for {url}' * 100,  # ~2KB content
                'markdown': f'# Mock Title {i}\n\nMock content...',
                'extraction_method': 'firecrawl_ai'
            })
        return results

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """Monitor system performance during tests"""
    
    def __init__(self):
        self.start_time = None
        self.start_cpu = None
        self.start_memory = None
        self.peak_memory = 0
        self.peak_cpu = 0
    
    def start(self):
        """Start monitoring"""
        self.start_time = time.time()
        self.start_cpu = psutil.cpu_percent()
        self.start_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
        self.peak_memory = self.start_memory
        self.peak_cpu = self.start_cpu
    
    def update(self):
        """Update peak measurements"""
        current_cpu = psutil.cpu_percent()
        current_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
        
        self.peak_cpu = max(self.peak_cpu, current_cpu)
        self.peak_memory = max(self.peak_memory, current_memory)
    
    def get_stats(self):
        """Get performance statistics"""
        end_time = time.time()
        end_memory = psutil.virtual_memory().used / 1024 / 1024  # MB
        
        return {
            'duration_seconds': end_time - self.start_time,
            'peak_cpu_percent': self.peak_cpu,
            'peak_memory_mb': self.peak_memory,
            'memory_delta_mb': end_memory - self.start_memory,
            'avg_cpu_percent': (self.start_cpu + psutil.cpu_percent()) / 2
        }


async def test_wayback_performance(domain: str, max_pages: int = 100):
    """Test Wayback Machine approach performance"""
    print(f"\nğŸ“Š Testing Wayback Machine: {domain} ({max_pages} pages max)")
    
    monitor = PerformanceMonitor()
    monitor.start()
    
    try:
        # Step 1: Fetch CDX records (parallel)
        start_fetch = time.time()
        records, stats = await fetch_cdx_records_parallel(
            domain_name=domain,
            from_date="20230101",
            to_date="20230201", 
            max_pages=5,  # Limit to avoid overwhelming in test
            filter_list_pages=True
        )
        fetch_time = time.time() - start_fetch
        monitor.update()
        
        # Step 2: Content extraction (sample)
        extraction_results = []
        extraction_start = time.time()
        
        # Extract content from first few records for timing
        for i, record in enumerate(records[:10]):  # Sample 10 for timing
            try:
                extracted = await extract_content_from_record(record)
                extraction_results.append({
                    'url': record.original_url,
                    'title': extracted.title,
                    'content_length': len(extracted.text),
                    'word_count': extracted.word_count,
                    'extraction_method': extracted.extraction_method
                })
                monitor.update()
            except Exception as e:
                logger.warning(f"Extraction failed for {record.original_url}: {e}")
                continue
        
        extraction_time = time.time() - extraction_start
        
        perf_stats = monitor.get_stats()
        
        return {
            'approach': 'wayback_machine',
            'domain': domain,
            'cdx_records_found': len(records),
            'content_extracted': len(extraction_results),
            'fetch_time': fetch_time,
            'extraction_time': extraction_time,
            'total_time': perf_stats['duration_seconds'],
            'peak_memory_mb': perf_stats['peak_memory_mb'],
            'peak_cpu_percent': perf_stats['peak_cpu_percent'],
            'memory_delta_mb': perf_stats['memory_delta_mb'],
            'pages_per_second': len(records) / perf_stats['duration_seconds'],
            'stats': stats,
            'sample_results': extraction_results[:3]  # First 3 for display
        }
        
    except Exception as e:
        return {
            'approach': 'wayback_machine',
            'domain': domain,
            'error': str(e),
            'total_time': monitor.get_stats()['duration_seconds']
        }


async def test_firecrawl_performance(domain: str, max_pages: int = 100):
    """Test Firecrawl approach performance (simulated)"""
    print(f"\nğŸ“Š Testing Firecrawl (Simulated): {domain} ({max_pages} pages max)")
    
    monitor = PerformanceMonitor()
    monitor.start()
    
    try:
        # Simulate getting URLs to scrape (in real scenario, you'd get these from sitemaps, etc.)
        sample_urls = [
            f"https://{domain}/page-{i}" for i in range(1, min(max_pages, 21))  # Limit to 20 for demo
        ]
        
        # Simulate Firecrawl scraping
        firecrawl_client = MockFirecrawlClient()
        results = await firecrawl_client.scrape_pages(sample_urls, max_pages=10)  # Limit for demo
        
        monitor.update()
        perf_stats = monitor.get_stats()
        
        return {
            'approach': 'firecrawl',
            'domain': domain,
            'urls_processed': len(results),
            'total_time': perf_stats['duration_seconds'],
            'peak_memory_mb': perf_stats['peak_memory_mb'],
            'peak_cpu_percent': perf_stats['peak_cpu_percent'],
            'memory_delta_mb': perf_stats['memory_delta_mb'],
            'pages_per_second': len(results) / perf_stats['duration_seconds'],
            'sample_results': results[:3]  # First 3 for display
        }
        
    except Exception as e:
        return {
            'approach': 'firecrawl',
            'domain': domain,
            'error': str(e),
            'total_time': monitor.get_stats()['duration_seconds']
        }


def calculate_costs(results, approach):
    """Calculate estimated costs for each approach"""
    pages = results.get('cdx_records_found', results.get('urls_processed', 0))
    
    if approach == 'wayback_machine':
        # Infrastructure costs
        cpu_hours = results['total_time'] / 3600
        bandwidth_gb = pages * 0.05 / 1024  # 50KB average per page
        storage_gb = bandwidth_gb  # Store extracted content
        
        cpu_cost = cpu_hours * 0.05  # $0.05 per CPU hour
        bandwidth_cost = bandwidth_gb * 0.10  # $0.10 per GB
        storage_cost = storage_gb * 0.02  # $0.02 per GB per month
        
        total_cost = cpu_cost + bandwidth_cost + storage_cost
        
        return {
            'cpu_cost': cpu_cost,
            'bandwidth_cost': bandwidth_cost,
            'storage_cost': storage_cost,
            'total_cost': total_cost,
            'cost_per_page': total_cost / pages if pages > 0 else 0
        }
    
    elif approach == 'firecrawl':
        # SaaS costs
        api_cost = pages * 0.003  # $0.003 per page (estimated)
        infrastructure_cost = 0.01  # Minimal infrastructure
        
        total_cost = api_cost + infrastructure_cost
        
        return {
            'api_cost': api_cost,
            'infrastructure_cost': infrastructure_cost,
            'total_cost': total_cost,
            'cost_per_page': total_cost / pages if pages > 0 else 0
        }


def print_comparison_results(wayback_results, firecrawl_results):
    """Print detailed comparison results"""
    print("\n" + "="*80)
    print("ğŸ“ˆ PERFORMANCE COMPARISON RESULTS")
    print("="*80)
    
    # Performance metrics
    print(f"\nâš¡ SPEED COMPARISON")
    print(f"{'Metric':<25} {'Wayback Machine':<20} {'Firecrawl':<20} {'Winner':<15}")
    print("-" * 80)
    
    wb_pages = wayback_results.get('cdx_records_found', 0)
    fc_pages = firecrawl_results.get('urls_processed', 0)
    
    wb_time = wayback_results.get('total_time', 0)
    fc_time = firecrawl_results.get('total_time', 0)
    
    wb_pps = wayback_results.get('pages_per_second', 0)
    fc_pps = firecrawl_results.get('pages_per_second', 0)
    
    print(f"{'Pages processed':<25} {wb_pages:<20} {fc_pages:<20} {'WB' if wb_pages > fc_pages else 'FC':<15}")
    print(f"{'Total time (s)':<25} {wb_time:<20.2f} {fc_time:<20.2f} {'WB' if wb_time < fc_time else 'FC':<15}")
    print(f"{'Pages/second':<25} {wb_pps:<20.2f} {fc_pps:<20.2f} {'WB' if wb_pps > fc_pps else 'FC':<15}")
    
    # Resource usage
    print(f"\nğŸ’¾ RESOURCE USAGE")
    print(f"{'Metric':<25} {'Wayback Machine':<20} {'Firecrawl':<20} {'Winner':<15}")
    print("-" * 80)
    
    wb_memory = wayback_results.get('peak_memory_mb', 0)
    fc_memory = firecrawl_results.get('peak_memory_mb', 0)
    
    wb_cpu = wayback_results.get('peak_cpu_percent', 0)
    fc_cpu = firecrawl_results.get('peak_cpu_percent', 0)
    
    print(f"{'Peak Memory (MB)':<25} {wb_memory:<20.1f} {fc_memory:<20.1f} {'FC' if fc_memory < wb_memory else 'WB':<15}")
    print(f"{'Peak CPU (%)':<25} {wb_cpu:<20.1f} {fc_cpu:<20.1f} {'FC' if fc_cpu < wb_cpu else 'WB':<15}")
    
    # Cost analysis
    print(f"\nğŸ’° COST ANALYSIS")
    wb_costs = calculate_costs(wayback_results, 'wayback_machine')
    fc_costs = calculate_costs(firecrawl_results, 'firecrawl')
    
    print(f"{'Cost Component':<25} {'Wayback Machine':<20} {'Firecrawl':<20}")
    print("-" * 65)
    print(f"{'Total Cost':<25} ${wb_costs['total_cost']:<19.4f} ${fc_costs['total_cost']:<19.4f}")
    print(f"{'Cost per Page':<25} ${wb_costs['cost_per_page']:<19.6f} ${fc_costs['cost_per_page']:<19.6f}")
    
    cost_ratio = fc_costs['total_cost'] / wb_costs['total_cost'] if wb_costs['total_cost'] > 0 else 0
    print(f"\nğŸ† Wayback Machine is {cost_ratio:.1f}x cheaper than Firecrawl")
    
    # Quality comparison (simulated)
    print(f"\nğŸ¯ QUALITY COMPARISON")
    print("Wayback Machine:")
    if 'sample_results' in wayback_results:
        for result in wayback_results['sample_results']:
            print(f"  â€¢ {result['title'][:50]}... ({result['word_count']} words)")
    
    print("\nFirecrawl (Simulated):")
    if 'sample_results' in firecrawl_results:
        for result in firecrawl_results['sample_results']:
            print(f"  â€¢ {result['title']} (AI-extracted)")
    
    # Recommendations
    print(f"\nğŸ¯ RECOMMENDATIONS")
    print(f"{'Use Case':<30} {'Recommended Approach':<20} {'Reason'}")
    print("-" * 70)
    print(f"{'Historical data (OSINT)':<30} {'Wayback Machine':<20} {'Unique historical access'}")
    print(f"{'High-volume processing':<30} {'Wayback Machine':<20} {'Cost & speed advantages'}")
    print(f"{'Current content analysis':<30} {'Firecrawl':<20} {'Live data access'}")
    print(f"{'Complex data extraction':<30} {'Firecrawl':<20} {'AI-powered quality'}")
    print(f"{'Budget-sensitive projects':<30} {'Wayback Machine':<20} {f'{cost_ratio:.0f}x cost savings'}")


async def run_performance_comparison():
    """Run comprehensive performance comparison"""
    print("ğŸš€ Starting Performance Comparison: Wayback Machine vs Firecrawl")
    print("="*80)
    
    # Test domain (using a domain likely to have archived content)
    test_domain = "httpbin.org"  # Well-known testing service with archived content
    max_pages = 50
    
    print(f"Test Configuration:")
    print(f"  Domain: {test_domain}")
    print(f"  Max pages: {max_pages}")
    print(f"  Test date range: 2023-01-01 to 2023-02-01")
    
    # Run tests
    wayback_results = await test_wayback_performance(test_domain, max_pages)
    firecrawl_results = await test_firecrawl_performance(test_domain, max_pages)
    
    # Display results
    print_comparison_results(wayback_results, firecrawl_results)
    
    # Summary
    print(f"\nğŸ“ EXECUTIVE SUMMARY")
    print("="*50)
    
    if 'error' not in wayback_results and 'error' not in firecrawl_results:
        wb_pps = wayback_results.get('pages_per_second', 0)
        fc_pps = firecrawl_results.get('pages_per_second', 0)
        speed_ratio = wb_pps / fc_pps if fc_pps > 0 else 0
        
        wb_costs = calculate_costs(wayback_results, 'wayback_machine')
        fc_costs = calculate_costs(firecrawl_results, 'firecrawl')
        cost_ratio = fc_costs['total_cost'] / wb_costs['total_cost'] if wb_costs['total_cost'] > 0 else 0
        
        print(f"âœ… Wayback Machine is {speed_ratio:.1f}x faster")
        print(f"âœ… Wayback Machine is {cost_ratio:.1f}x cheaper")
        print(f"âœ… Wayback Machine provides unique historical data")
        print(f"âš ï¸  Firecrawl provides higher extraction quality")
        print(f"âš ï¸  Firecrawl accesses current content")
        
        print(f"\nğŸ¯ VERDICT: For OSINT and historical analysis, Wayback Machine is superior")
        print(f"   For current content with complex extraction needs, consider Firecrawl")
    
    else:
        if 'error' in wayback_results:
            print(f"âŒ Wayback Machine test failed: {wayback_results['error']}")
        if 'error' in firecrawl_results:
            print(f"âŒ Firecrawl test failed: {firecrawl_results['error']}")


if __name__ == "__main__":
    print("Performance Comparison Test")
    print("This test compares Wayback Machine vs Firecrawl approaches")
    print("Note: Firecrawl is simulated since it requires API access")
    print()
    
    try:
        asyncio.run(run_performance_comparison())
    except KeyboardInterrupt:
        print("\nâŒ Test interrupted by user")
    except Exception as e:
        print(f"âŒ Test failed with error: {str(e)}")
        sys.exit(1)