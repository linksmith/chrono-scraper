# Optimized Docker Compose Configuration for 8-core/16GB System
# Resource allocation strategy for Chrono Scraper FastAPI application

volumes:
  postgres_data:
  meilisearch_data:
  redis_data:

networks:
  chrono_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

services:
  # =============================================================================
  # DATA LAYER - Critical foundation services
  # =============================================================================
  
  # PostgreSQL Database - Primary data store
  postgres:
    image: postgres:17-alpine
    container_name: chrono_postgres
    # Resource allocation: 1.5GB RAM, 1 CPU core reserved
    deploy:
      resources:
        limits:
          memory: 1.5G
          cpus: '2.0'  # Can burst to 2 cores
        reservations:
          memory: 800M
          cpus: '1.0'  # 1 core reserved
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - /dev/shm:/dev/shm:rw  # Shared memory optimization
    environment:
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      # PostgreSQL performance tuning for 16GB system
      - POSTGRES_SHARED_BUFFERS=384MB  # 25% of allocated memory
      - POSTGRES_EFFECTIVE_CACHE_SIZE=1GB  # 75% of allocated memory
      - POSTGRES_WORK_MEM=32MB  # Increased for complex queries
      - POSTGRES_MAINTENANCE_WORK_MEM=128MB
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRES_WAL_BUFFERS=16MB
      - POSTGRES_DEFAULT_STATISTICS_TARGET=100
      - POSTGRES_RANDOM_PAGE_COST=1.1  # SSD optimization
      - POSTGRES_EFFECTIVE_IO_CONCURRENCY=200
    ports:
      - "5435:5432"
    networks:
      - chrono_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U chrono_scraper"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    command: >
      postgres
      -c shared_buffers=384MB
      -c effective_cache_size=1GB
      -c work_mem=32MB
      -c maintenance_work_mem=128MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c max_connections=100
      -c shared_preload_libraries=pg_stat_statements
      -c log_statement=all
      -c log_min_duration_statement=1000

  # Redis Cache & Queue - High-performance caching and task queue
  redis:
    image: redis:7-alpine
    container_name: chrono_redis
    # Resource allocation: 512MB RAM, 0.5 CPU cores
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.5'
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      - chrono_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: >
      redis-server
      --maxmemory 384mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
      --appendonly yes
      --appendfsync everysec
      --tcp-keepalive 300
      --timeout 0

  # =============================================================================
  # SEARCH ENGINE - Meilisearch for full-text search
  # =============================================================================
  
  meilisearch:
    image: getmeili/meilisearch:v1.16
    container_name: chrono_meilisearch
    # Resource allocation: 1GB RAM, 1 CPU core
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.5'
        reservations:
          memory: 512M
          cpus: '1.0'
    volumes:
      - meilisearch_data:/meili_data
    environment:
      - MEILI_MASTER_KEY=${MEILISEARCH_MASTER_KEY:-}
      - MEILI_ENV=development
      - MEILI_NO_ANALYTICS=true
      - MEILI_MAX_INDEXING_MEMORY=512MB
      - MEILI_MAX_INDEXING_THREADS=2
      - MEILI_HTTP_PAYLOAD_SIZE_LIMIT=100MB
    ports:
      - "7700:7700"
    networks:
      - chrono_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7700/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      - redis

  # =============================================================================
  # BROWSER AUTOMATION - Firecrawl services for content extraction
  # =============================================================================
  
  # Playwright Service - Heavy browser automation
  firecrawl-playwright:
    build:
      context: ./external/firecrawl/apps/playwright-service-ts
      dockerfile: Dockerfile
    container_name: chrono_firecrawl_playwright
    # Resource allocation: 3GB RAM, 1.5 CPU cores (browser automation is memory-intensive)
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: '2.0'
        reservations:
          memory: 1.5G
          cpus: '1.0'
    environment:
      - PORT=3000
      - HOST=0.0.0.0
      - PROXY_SERVER=${FIRECRAWL_PROXY_SERVER:-}
      - PROXY_USERNAME=${FIRECRAWL_PROXY_USERNAME:-}
      - PROXY_PASSWORD=${FIRECRAWL_PROXY_PASSWORD:-}
      - BLOCK_MEDIA=${FIRECRAWL_BLOCK_MEDIA:-true}
      # Memory optimization for Node.js and browsers
      - NODE_OPTIONS=--max-old-space-size=2048
      - PLAYWRIGHT_BROWSERS_PATH=/tmp/.cache
      # Performance tuning for browser automation
      - DEFAULT_TIMEOUT=120000
      - MAX_TIMEOUT=120000
      - MAX_CONCURRENT_SESSIONS=3  # Limit concurrent browser sessions
      - BROWSER_POOL_SIZE=2  # Browser instance pool
    ports:
      - "3000:3000"
    shm_size: 2gb  # Shared memory for browser processes
    restart: unless-stopped
    networks:
      - chrono_network
    healthcheck:
      test: ["CMD-SHELL", "node -e \"fetch('http://localhost:3000/health').then(r=>{if(r.status===200)process.exit(0);process.exit(1)}).catch(()=>process.exit(1))\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    sysctls:
      - net.ipv4.ip_local_port_range=1024 65535
      - net.core.somaxconn=1024

  # Firecrawl API Service
  firecrawl-api:
    build:
      context: ./external/firecrawl/apps/api
      dockerfile: Dockerfile
    container_name: chrono_firecrawl_api
    # Resource allocation: 1GB RAM, 1 CPU core
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.5'
        reservations:
          memory: 512M
          cpus: '0.5'
    depends_on:
      - redis
      - firecrawl-playwright
    environment:
      - HOST=0.0.0.0
      - PORT=3002
      - INTERNAL_PORT=3002
      - ENV=local
      - FLY_PROCESS_GROUP=app
      - REDIS_URL=redis://redis:6379
      - REDIS_RATE_LIMIT_URL=redis://redis:6379
      - PLAYWRIGHT_MICROSERVICE_URL=http://firecrawl-playwright:3000/scrape
      - USE_DB_AUTHENTICATION=false
      - LOGGING_LEVEL=${FIRECRAWL_LOGGING_LEVEL:-info}
      - OPENAI_API_KEY=${FIRECRAWL_OPENAI_API_KEY:-}
      - OPENAI_BASE_URL=${FIRECRAWL_OPENAI_BASE_URL:-}
      - MODEL_NAME=${FIRECRAWL_MODEL_NAME:-gpt-4o-mini}
      - MODEL_EMBEDDING_NAME=${FIRECRAWL_MODEL_EMBEDDING_NAME:-text-embedding-3-small}
      - OPENROUTER_API_KEY=${FIRECRAWL_OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${FIRECRAWL_OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - SERPER_API_KEY=${FIRECRAWL_SERPER_API_KEY:-}
      - SEARCHAPI_API_KEY=${FIRECRAWL_SEARCHAPI_API_KEY:-}
      - PROXY_SERVER=${FIRECRAWL_PROXY_SERVER:-}
      - PROXY_USERNAME=${FIRECRAWL_PROXY_USERNAME:-}
      - PROXY_PASSWORD=${FIRECRAWL_PROXY_PASSWORD:-}
      - BULL_AUTH_KEY=${FIRECRAWL_BULL_AUTH_KEY:-dev-auth-key}
      - TEST_API_KEY=${FIRECRAWL_TEST_API_KEY:-fc-test-key}
      # Performance tuning
      - NODE_OPTIONS=--max-old-space-size=768
      - UV_THREADPOOL_SIZE=16
    ports:
      - "3002:3002"
    command: ["pnpm", "run", "start:production"]
    restart: unless-stopped
    networks:
      - chrono_network
    healthcheck:
      test: ["CMD-SHELL", "node -e \"fetch('http://localhost:3002/v0/health/liveness').then(r=>{if(r.status===200)process.exit(0);process.exit(1)}).catch(()=>process.exit(1))\""]
      interval: 30s
      timeout: 10s
      retries: 3

  # Firecrawl Worker Service
  firecrawl-worker:
    build:
      context: ./external/firecrawl/apps/api
      dockerfile: Dockerfile
    container_name: chrono_firecrawl_worker
    # Resource allocation: 1.5GB RAM, 1 CPU core
    deploy:
      resources:
        limits:
          memory: 1.5G
          cpus: '1.5'
        reservations:
          memory: 768M
          cpus: '1.0'
    depends_on:
      - redis
      - firecrawl-playwright
      - firecrawl-api
    environment:
      - ENV=local
      - FLY_PROCESS_GROUP=worker
      - REDIS_URL=redis://redis:6379
      - REDIS_RATE_LIMIT_URL=redis://redis:6379
      - PLAYWRIGHT_MICROSERVICE_URL=http://firecrawl-playwright:3000/scrape
      - USE_DB_AUTHENTICATION=false
      - LOGGING_LEVEL=${FIRECRAWL_LOGGING_LEVEL:-info}
      - OPENAI_API_KEY=${FIRECRAWL_OPENAI_API_KEY:-}
      - OPENAI_BASE_URL=${FIRECRAWL_OPENAI_BASE_URL:-}
      - MODEL_NAME=${FIRECRAWL_MODEL_NAME:-gpt-4o-mini}
      - MODEL_EMBEDDING_NAME=${FIRECRAWL_MODEL_EMBEDDING_NAME:-text-embedding-3-small}
      - OPENROUTER_API_KEY=${FIRECRAWL_OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${FIRECRAWL_OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - SERPER_API_KEY=${FIRECRAWL_SERPER_API_KEY:-}
      - SEARCHAPI_API_KEY=${FIRECRAWL_SEARCHAPI_API_KEY:-}
      - PROXY_SERVER=${FIRECRAWL_PROXY_SERVER:-}
      - PROXY_USERNAME=${FIRECRAWL_PROXY_USERNAME:-}
      - PROXY_PASSWORD=${FIRECRAWL_PROXY_PASSWORD:-}
      - BLOCK_MEDIA=${FIRECRAWL_BLOCK_MEDIA:-true}
      # Worker performance tuning
      - NODE_OPTIONS=--max-old-space-size=1024
      - WORKER_CONCURRENCY=4  # Limit concurrent processing
    command: ["pnpm", "run", "workers"]
    restart: unless-stopped
    networks:
      - chrono_network

  # =============================================================================
  # APPLICATION LAYER - Backend and task processing
  # =============================================================================
  
  # FastAPI Backend - Main application server
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_backend
    # Resource allocation: 1GB RAM, 1 CPU core
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.5'
        reservations:
          memory: 512M
          cpus: '1.0'
    volumes:
      - ./backend:/app:z
    ports:
      - "8000:8000"
    environment:
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      - REDIS_HOST=redis
      - MEILISEARCH_HOST=http://meilisearch:7700
      - MEILISEARCH_MASTER_KEY=RuvEMt9LztgYqdfqRFmZbT52uysNrt73ps57RZ2PRd53kjWxe2qiv9kadk9EiV5k
      - BACKEND_CORS_ORIGINS=["http://localhost:3000","http://localhost:5173","http://dl:3000","http://dl:5173"]
      - ENVIRONMENT=development
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - SMTP_HOST=mailpit
      - SMTP_PORT=1025
      - SMTP_TLS=false
      - EMAILS_FROM_EMAIL=noreply@chrono-scraper.local
      - EMAILS_FROM_NAME=Chrono Scraper Dev
      - FIRECRAWL_LOCAL_URL=http://firecrawl-api:3002
      - FIRECRAWL_BASE_URL=http://firecrawl-api:3002
      - FIRECRAWL_API_KEY=${FIRECRAWL_TEST_API_KEY:-fc-test-key-for-local-development}
      - FIRECRAWL_MODE=local
      # FastAPI performance tuning
      - UVICORN_WORKERS=2
      - UVICORN_WORKER_CONNECTIONS=1000
      - UVICORN_BACKLOG=2048
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      meilisearch:
        condition: service_healthy
      firecrawl-api:
        condition: service_healthy
    networks:
      - chrono_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery Worker - Background task processing
  celery_worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_celery_worker
    # Resource allocation: 2GB RAM, 2 CPU cores (high for scraping tasks)
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.5'
        reservations:
          memory: 1G
          cpus: '1.5'
      replicas: 1  # Single worker with higher concurrency
    command: >
      celery -A app.tasks.celery_app worker 
      --loglevel=info 
      --queues=scraping,indexing,quick,celery 
      --concurrency=6 
      --prefetch-multiplier=2
      --max-tasks-per-child=50
      --max-memory-per-child=400000
    volumes:
      - ./backend:/app:z
    environment:
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      - REDIS_HOST=redis
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - ENVIRONMENT=development
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - SMTP_HOST=mailpit
      - SMTP_PORT=1025
      - SMTP_TLS=false
      - EMAILS_FROM_EMAIL=noreply@chrono-scraper.local
      - EMAILS_FROM_NAME=Chrono Scraper Dev
      - FIRECRAWL_LOCAL_URL=http://firecrawl-api:3002
      - FIRECRAWL_BASE_URL=http://firecrawl-api:3002
      - FIRECRAWL_API_KEY=${FIRECRAWL_TEST_API_KEY:-fc-test-key-for-local-development}
      - FIRECRAWL_MODE=local
      # Memory management for workers
      - PYTHONHASHSEED=0
      - PYTHONUNBUFFERED=1
      - OMP_NUM_THREADS=2
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      firecrawl-api:
        condition: service_healthy
    networks:
      - chrono_network
    restart: unless-stopped

  # Celery Beat Scheduler
  celery_beat:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_celery_beat
    # Resource allocation: 256MB RAM, 0.25 CPU cores (minimal scheduler)
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
    command: celery -A app.tasks.celery_app beat --loglevel=info
    volumes:
      - ./backend:/app:z
    environment:
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      - REDIS_HOST=redis
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - ENVIRONMENT=development
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - SMTP_HOST=mailpit
      - SMTP_PORT=1025
      - SMTP_TLS=false
      - EMAILS_FROM_EMAIL=noreply@chrono-scraper.local
      - EMAILS_FROM_NAME=Chrono Scraper Dev
      - FIRECRAWL_LOCAL_URL=http://firecrawl-api:3002
      - FIRECRAWL_BASE_URL=http://firecrawl-api:3002
      - FIRECRAWL_API_KEY=${FIRECRAWL_TEST_API_KEY:-fc-test-key-for-local-development}
      - FIRECRAWL_MODE=local
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      firecrawl-api:
        condition: service_healthy
    networks:
      - chrono_network
    restart: unless-stopped

  # =============================================================================
  # FRONTEND - SvelteKit application
  # =============================================================================
  
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: chrono_frontend
    # Resource allocation: 768MB RAM, 0.5 CPU cores (increased from 512MB to prevent memory pressure)
    deploy:
      resources:
        limits:
          memory: 768M  # Increased from 512M to prevent memory pressure during development
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.5'
    volumes:
      - ./frontend:/app:z
      - /app/node_modules
    ports:
      - "5173:5173"
    environment:
      - PUBLIC_API_URL=http://localhost:8000
      - PUBLIC_MEILISEARCH_HOST=http://localhost:7700
      - PUBLIC_MEILISEARCH_KEY=VEnx9yhSY9jLjUeNfhCQ3Yv92MhELNAFbFus5HfTjbX4uU3yWo7ycSzeWmDha5vE
      - BACKEND_URL=http://backend:8000
      - API_BASE_URL=http://backend:8000
      # Node.js performance tuning
      - NODE_OPTIONS=--max-old-space-size=1024
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - chrono_network
    restart: unless-stopped

  # =============================================================================
  # MONITORING & UTILITIES
  # =============================================================================
  
  # Flower - Celery monitoring
  flower:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_flower
    # Resource allocation: 256MB RAM, 0.25 CPU cores
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
    command: celery -A app.tasks.celery_app flower --port=5555
    volumes:
      - ./backend:/app:z
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - chrono_network
    restart: unless-stopped

  # Mailpit - Email testing
  mailpit:
    image: axllent/mailpit:latest
    container_name: chrono_mailpit
    # Resource allocation: 128MB RAM, 0.1 CPU cores
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'
    ports:
      - "1025:1025"
      - "8025:8025"
    networks:
      - chrono_network
    restart: unless-stopped