volumes:
  postgres_data:
  meilisearch_data:
  redis_data:
  backup_data:
  backup_logs:

services:
  # FastAPI Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_backend
    volumes:
      - ./backend:/app:z
      - backup_data:/app/backups:z
      - backup_logs:/app/logs:z
    ports:
      - "8000:8000"
    environment:
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      - REDIS_HOST=redis
      - MEILISEARCH_HOST=http://meilisearch:7700
      - MEILISEARCH_MASTER_KEY=RuvEMt9LztgYqdfqRFmZbT52uysNrt73ps57RZ2PRd53kjWxe2qiv9kadk9EiV5k
      - BACKEND_CORS_ORIGINS=["http://localhost:3000","http://localhost:5173","http://dl:3000","http://dl:5173"]
      - ENVIRONMENT=development
      # OpenRouter configuration for LLM-based features (user evaluation, project naming)
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - SMTP_HOST=mailpit
      - SMTP_PORT=1025
      - SMTP_TLS=false
      - EMAILS_FROM_EMAIL=noreply@chrono-scraper.local
      - EMAILS_FROM_NAME=Chrono Scraper Dev
      # Firecrawl Local Configuration
      - FIRECRAWL_LOCAL_URL=http://firecrawl-api:3002
      - FIRECRAWL_BASE_URL=http://firecrawl-api:3002
      - FIRECRAWL_API_KEY=${FIRECRAWL_TEST_API_KEY:-fc-test-key-for-local-development}
      - FIRECRAWL_MODE=local
      # Backup System Configuration
      - BACKUP_ENABLED=${BACKUP_ENABLED:-true}
      - BACKUP_LOCAL_ENABLED=${BACKUP_LOCAL_ENABLED:-true}
      - BACKUP_LOCAL_PATH=${BACKUP_LOCAL_PATH:-/app/backups}
      - BACKUP_ENCRYPTION_KEY=${BACKUP_ENCRYPTION_KEY:-}
      - BACKUP_LOG_LEVEL=${BACKUP_LOG_LEVEL:-INFO}
      - BACKUP_NOTIFICATIONS_ENABLED=${BACKUP_NOTIFICATIONS_ENABLED:-true}
      - BACKUP_EMAIL_ENABLED=${BACKUP_EMAIL_ENABLED:-false}
      - BACKUP_SLACK_ENABLED=${BACKUP_SLACK_ENABLED:-false}
      - BACKUP_AWS_ENABLED=${BACKUP_AWS_ENABLED:-false}
      - BACKUP_GCS_ENABLED=${BACKUP_GCS_ENABLED:-false}
      - BACKUP_AZURE_ENABLED=${BACKUP_AZURE_ENABLED:-false}
      - BACKUP_SFTP_ENABLED=${BACKUP_SFTP_ENABLED:-false}
    depends_on:
      - postgres
      - redis
      - meilisearch
      - firecrawl-api
    networks:
      - chrono_network
    restart: unless-stopped

  # SvelteKit Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: chrono_frontend
    volumes:
      - ./frontend:/app:z
      - /app/node_modules
    ports:
      - "5173:5173"
    environment:
      - PUBLIC_API_URL=http://localhost:8000
      - PUBLIC_MEILISEARCH_HOST=http://localhost:7700
      - PUBLIC_MEILISEARCH_KEY=VEnx9yhSY9jLjUeNfhCQ3Yv92MhELNAFbFus5HfTjbX4uU3yWo7ycSzeWmDha5vE
      - BACKEND_URL=http://backend:8000
      - API_BASE_URL=http://backend:8000
    depends_on:
      - backend
    networks:
      - chrono_network
    restart: unless-stopped

  # PostgreSQL Database
  postgres:
    image: postgres:17-alpine
    container_name: chrono_postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
    ports:
      - "5435:5432"
    networks:
      - chrono_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U chrono_scraper"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Cache & Queue
  redis:
    image: redis:7-alpine
    container_name: chrono_redis
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      - chrono_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Meilisearch
  meilisearch:
    image: getmeili/meilisearch:v1.16
    container_name: chrono_meilisearch
    volumes:
      - meilisearch_data:/meili_data
    environment:
      - MEILI_MASTER_KEY=RuvEMt9LztgYqdfqRFmZbT52uysNrt73ps57RZ2PRd53kjWxe2qiv9kadk9EiV5k
      - MEILI_ENV=development
      - MEILI_NO_ANALYTICS=true
    ports:
      - "7700:7700"
    networks:
      - chrono_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7700/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Celery Worker
  celery_worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_celery_worker
    command: celery -A app.tasks.celery_app worker --loglevel=info --queues=scraping,indexing,quick,celery,backup
    volumes:
      - ./backend:/app:z
      - backup_data:/app/backups:z
      - backup_logs:/app/logs:z
    environment:
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      - REDIS_HOST=redis
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - ENVIRONMENT=development
      # OpenRouter configuration for any background tasks that may use LLMs
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - SMTP_HOST=mailpit
      - SMTP_PORT=1025
      - SMTP_TLS=false
      - EMAILS_FROM_EMAIL=noreply@chrono-scraper.local
      - EMAILS_FROM_NAME=Chrono Scraper Dev
      # Firecrawl Local Configuration
      - FIRECRAWL_LOCAL_URL=http://firecrawl-api:3002
      - FIRECRAWL_BASE_URL=http://firecrawl-api:3002
      - FIRECRAWL_API_KEY=${FIRECRAWL_TEST_API_KEY:-fc-test-key-for-local-development}
      - FIRECRAWL_MODE=local
      # Backup System Configuration
      - BACKUP_ENABLED=${BACKUP_ENABLED:-true}
      - BACKUP_LOCAL_ENABLED=${BACKUP_LOCAL_ENABLED:-true}
      - BACKUP_LOCAL_PATH=${BACKUP_LOCAL_PATH:-/app/backups}
      - BACKUP_ENCRYPTION_KEY=${BACKUP_ENCRYPTION_KEY:-}
      - BACKUP_LOG_LEVEL=${BACKUP_LOG_LEVEL:-INFO}
    depends_on:
      - postgres
      - redis
      - firecrawl-api
    networks:
      - chrono_network
    restart: unless-stopped

  # Celery Beat Scheduler
  celery_beat:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_celery_beat
    command: celery -A app.tasks.celery_app beat --loglevel=info
    volumes:
      - ./backend:/app:z
    environment:
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      - REDIS_HOST=redis
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - ENVIRONMENT=development
      # OpenRouter configuration for any scheduled tasks that may use LLMs
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - SMTP_HOST=mailpit
      - SMTP_PORT=1025
      - SMTP_TLS=false
      - EMAILS_FROM_EMAIL=noreply@chrono-scraper.local
      - EMAILS_FROM_NAME=Chrono Scraper Dev
      # Firecrawl Local Configuration
      - FIRECRAWL_LOCAL_URL=http://firecrawl-api:3002
      - FIRECRAWL_BASE_URL=http://firecrawl-api:3002
      - FIRECRAWL_API_KEY=${FIRECRAWL_TEST_API_KEY:-fc-test-key-for-local-development}
      - FIRECRAWL_MODE=local
    depends_on:
      - postgres
      - redis
      - firecrawl-api
    networks:
      - chrono_network
    restart: unless-stopped

  # Flower - Celery Monitoring
  flower:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_flower
    command: celery -A app.tasks.celery_app flower --port=5555
    volumes:
      - ./backend:/app:z
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      - redis
    networks:
      - chrono_network
    restart: unless-stopped

  # Mailpit - Email Testing
  mailpit:
    image: axllent/mailpit:latest
    container_name: chrono_mailpit
    ports:
      - "1025:1025"
      - "8025:8025"
    networks:
      - chrono_network
    restart: unless-stopped

  # Firecrawl API Service
  firecrawl-api:
    build:
      context: ./external/firecrawl/apps/api
      dockerfile: Dockerfile
    container_name: chrono_firecrawl_api
    depends_on:
      - redis
      - firecrawl-playwright
    environment:
      # Core Firecrawl settings
      - HOST=0.0.0.0
      - PORT=3002
      - INTERNAL_PORT=3002
      - ENV=local
      - FLY_PROCESS_GROUP=app
      
      # Redis configuration - use chrono-scraper's Redis
      - REDIS_URL=redis://redis:6379
      - REDIS_RATE_LIMIT_URL=redis://redis:6379
      
      # Playwright service
      - PLAYWRIGHT_MICROSERVICE_URL=http://firecrawl-playwright:3000/scrape
      
      # Authentication (disabled for local development)
      - USE_DB_AUTHENTICATION=false
      
      # Logging
      - LOGGING_LEVEL=${FIRECRAWL_LOGGING_LEVEL:-info}
      
      # API Keys (load from environment file)
      - OPENAI_API_KEY=${FIRECRAWL_OPENAI_API_KEY:-}
      - OPENAI_BASE_URL=${FIRECRAWL_OPENAI_BASE_URL:-}
      - MODEL_NAME=${FIRECRAWL_MODEL_NAME:-gpt-4o-mini}
      - MODEL_EMBEDDING_NAME=${FIRECRAWL_MODEL_EMBEDDING_NAME:-text-embedding-3-small}
      
      # OpenRouter support (alternative to OpenAI)
      - OPENROUTER_API_KEY=${FIRECRAWL_OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${FIRECRAWL_OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      
      # Optional services
      - SERPER_API_KEY=${FIRECRAWL_SERPER_API_KEY:-}
      - SEARCHAPI_API_KEY=${FIRECRAWL_SEARCHAPI_API_KEY:-}
      
      # Proxy settings (inherit from chrono-scraper if needed)
      - PROXY_SERVER=${FIRECRAWL_PROXY_SERVER:-}
      - PROXY_USERNAME=${FIRECRAWL_PROXY_USERNAME:-}
      - PROXY_PASSWORD=${FIRECRAWL_PROXY_PASSWORD:-}
      
      # Development settings
      - BULL_AUTH_KEY=${FIRECRAWL_BULL_AUTH_KEY:-dev-auth-key}
      - TEST_API_KEY=${FIRECRAWL_TEST_API_KEY:-fc-test-key}
      - NUM_WORKERS_PER_QUEUE=${FIRECRAWL_NUM_WORKERS_PER_QUEUE:-8}
      - BLOCK_MEDIA=${FIRECRAWL_BLOCK_MEDIA:-true}
    ports:
      - "3002:3002"
    command: ["pnpm", "run", "start:production"]
    restart: unless-stopped
    networks:
      - chrono_network
    healthcheck:
      # Use node to avoid needing curl inside the container
      test: ["CMD-SHELL", "node -e \"fetch('http://localhost:3002/v0/health/liveness').then(r=>{if(r.status===200)process.exit(0);process.exit(1)}).catch(()=>process.exit(1))\""]
      interval: 30s
      timeout: 10s
      retries: 3

  # Firecrawl Worker Service
  firecrawl-worker:
    build:
      context: ./external/firecrawl/apps/api
      dockerfile: Dockerfile
    container_name: chrono_firecrawl_worker
    depends_on:
      - redis
      - firecrawl-playwright
      - firecrawl-api
    environment:
      # Core settings
      - ENV=local
      - FLY_PROCESS_GROUP=worker
      
      # Redis configuration
      - REDIS_URL=redis://redis:6379
      - REDIS_RATE_LIMIT_URL=redis://redis:6379
      
      # Playwright service
      - PLAYWRIGHT_MICROSERVICE_URL=http://firecrawl-playwright:3000/scrape
      
      # Authentication
      - USE_DB_AUTHENTICATION=false
      
      # Logging
      - LOGGING_LEVEL=${FIRECRAWL_LOGGING_LEVEL:-info}
      
      # API Keys
      - OPENAI_API_KEY=${FIRECRAWL_OPENAI_API_KEY:-}
      - OPENAI_BASE_URL=${FIRECRAWL_OPENAI_BASE_URL:-}
      - MODEL_NAME=${FIRECRAWL_MODEL_NAME:-gpt-4o-mini}
      - MODEL_EMBEDDING_NAME=${FIRECRAWL_MODEL_EMBEDDING_NAME:-text-embedding-3-small}
      
      # OpenRouter support (alternative to OpenAI)
      - OPENROUTER_API_KEY=${FIRECRAWL_OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${FIRECRAWL_OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      
      # Optional services
      - SERPER_API_KEY=${FIRECRAWL_SERPER_API_KEY:-}
      - SEARCHAPI_API_KEY=${FIRECRAWL_SEARCHAPI_API_KEY:-}
      
      # Proxy settings
      - PROXY_SERVER=${FIRECRAWL_PROXY_SERVER:-}
      - PROXY_USERNAME=${FIRECRAWL_PROXY_USERNAME:-}
      - PROXY_PASSWORD=${FIRECRAWL_PROXY_PASSWORD:-}
      - BLOCK_MEDIA=${FIRECRAWL_BLOCK_MEDIA:-true}
    command: ["pnpm", "run", "workers"]
    restart: unless-stopped
    networks:
      - chrono_network

  # Playwright Service for browser automation
  firecrawl-playwright:
    build:
      context: ./external/firecrawl/apps/playwright-service-ts
      dockerfile: Dockerfile
    container_name: chrono_firecrawl_playwright
    environment:
      - PORT=3000
      - HOST=0.0.0.0
      - PROXY_SERVER=${FIRECRAWL_PROXY_SERVER:-}
      - PROXY_USERNAME=${FIRECRAWL_PROXY_USERNAME:-}
      - PROXY_PASSWORD=${FIRECRAWL_PROXY_PASSWORD:-}
      - BLOCK_MEDIA=${FIRECRAWL_BLOCK_MEDIA:-true}  # Block media to reduce resource usage
      # Browser stability optimizations
      - NODE_OPTIONS=--max-old-space-size=2048
      # Fix browser path to match Dockerfile
      - PLAYWRIGHT_BROWSERS_PATH=/tmp/.cache
      # Extended timeouts for Wayback Machine (2 minutes)
      - DEFAULT_TIMEOUT=120000
      - MAX_TIMEOUT=120000
    ports:
      - "3000:3000"
    shm_size: 3gb  # Increased shared memory for browser processes
    restart: unless-stopped
    # Resource limits to prevent crashes
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 1G
    networks:
      - chrono_network
    healthcheck:
      # Call the dedicated health endpoint; avoid curl dependency
      test: ["CMD-SHELL", "node -e \"fetch('http://localhost:3000/health').then(r=>{if(r.status===200)process.exit(0);process.exit(1)}).catch(()=>process.exit(1))\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

networks:
  chrono_network:
    driver: bridge