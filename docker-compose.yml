volumes:
  postgres_data:
  meilisearch_data:
  redis_data:
  backup_data:
  backup_logs:

services:
  # FastAPI Backend - Optimized for robust extraction
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_backend
    volumes:
      - ./backend:/app:z
      - backup_data:/app/backups:z
      - backup_logs:/app/logs:z
    ports:
      - "8000:8000"
    environment:
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      - REDIS_HOST=redis
      - MEILISEARCH_HOST=http://meilisearch:7700
      - MEILISEARCH_MASTER_KEY=RuvEMt9LztgYqdfqRFmZbT52uysNrt73ps57RZ2PRd53kjWxe2qiv9kadk9EiV5k
      - BACKEND_CORS_ORIGINS=["http://localhost:3000","http://localhost:5173","http://dl:3000","http://dl:5173"]
      - ENVIRONMENT=development
      # OpenRouter configuration for LLM-based features (user evaluation, project naming)
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - SMTP_HOST=mailpit
      - SMTP_PORT=1025
      - SMTP_TLS=false
      - EMAILS_FROM_EMAIL=noreply@chrono-scraper.local
      - EMAILS_FROM_NAME=Chrono Scraper Dev
      # Robust extraction configuration
      - USE_INTELLIGENT_EXTRACTION_ONLY=true
      - INTELLIGENT_EXTRACTION_CONCURRENCY=15
      - ROBUST_EXTRACTION_ENABLED=true
      - EXTRACTION_TIMEOUT=45
      - EXTRACTION_CACHE_TTL=3600
      # Individual content extraction enabled
      - USE_INDIVIDUAL_EXTRACTION=true
      # Proxy Configuration for Archive.org access
      - USE_PROXY=${USE_PROXY:-false}
      - PROXY_SERVER=${PROXY_SERVER:-}
      - PROXY_USERNAME=${PROXY_USERNAME:-}
      - PROXY_PASSWORD=${PROXY_PASSWORD:-}
      # Backup System Configuration
      - BACKUP_ENABLED=${BACKUP_ENABLED:-true}
      - BACKUP_LOCAL_ENABLED=${BACKUP_LOCAL_ENABLED:-true}
      - BACKUP_LOCAL_PATH=${BACKUP_LOCAL_PATH:-/app/backups}
      - BACKUP_ENCRYPTION_KEY=${BACKUP_ENCRYPTION_KEY:-}
      - BACKUP_LOG_LEVEL=${BACKUP_LOG_LEVEL:-INFO}
      - BACKUP_NOTIFICATIONS_ENABLED=${BACKUP_NOTIFICATIONS_ENABLED:-true}
      - BACKUP_EMAIL_ENABLED=${BACKUP_EMAIL_ENABLED:-false}
      - BACKUP_SLACK_ENABLED=${BACKUP_SLACK_ENABLED:-false}
      - BACKUP_AWS_ENABLED=${BACKUP_AWS_ENABLED:-false}
      - BACKUP_GCS_ENABLED=${BACKUP_GCS_ENABLED:-false}
      - BACKUP_AZURE_ENABLED=${BACKUP_AZURE_ENABLED:-false}
      - BACKUP_SFTP_ENABLED=${BACKUP_SFTP_ENABLED:-false}
    depends_on:
      - postgres
      - redis
      - meilisearch
    networks:
      - chrono_network
    restart: unless-stopped
    # Resource allocation - redistributed from Firecrawl services
    deploy:
      resources:
        limits:
          memory: 2G  # Increased from 1G (using freed Firecrawl memory)
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'

  # SvelteKit Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: chrono_frontend
    volumes:
      - ./frontend:/app:z
      - /app/node_modules
    ports:
      - "5173:5173"
    environment:
      - PUBLIC_API_URL=http://localhost:8000
      - PUBLIC_MEILISEARCH_HOST=http://localhost:7700
      - PUBLIC_MEILISEARCH_KEY=VEnx9yhSY9jLjUeNfhCQ3Yv92MhELNAFbFus5HfTjbX4uU3yWo7ycSzeWmDha5vE
      - BACKEND_URL=http://backend:8000
      - API_BASE_URL=http://backend:8000
      - NODE_OPTIONS=--max-old-space-size=768
    depends_on:
      - backend
    networks:
      - chrono_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  # PostgreSQL Database - Enhanced for content processing
  postgres:
    image: postgres:17-alpine
    container_name: chrono_postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      # PostgreSQL optimization for content processing
      - POSTGRES_SHARED_BUFFERS=512MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=1536MB
      - POSTGRES_MAINTENANCE_WORK_MEM=128MB
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRES_WAL_BUFFERS=16MB
      - POSTGRES_DEFAULT_STATISTICS_TARGET=100
      - POSTGRES_RANDOM_PAGE_COST=1.1
      - POSTGRES_EFFECTIVE_IO_CONCURRENCY=200
      - POSTGRES_WORK_MEM=8MB
      - POSTGRES_HUGE_PAGES=off
      - POSTGRES_MIN_WAL_SIZE=1GB
      - POSTGRES_MAX_WAL_SIZE=4GB
    ports:
      - "5435:5432"
    networks:
      - chrono_network
    restart: unless-stopped
    # Enhanced resource allocation
    deploy:
      resources:
        limits:
          memory: 3G  # Increased from 2G (using freed Firecrawl memory)
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U chrono_scraper"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Cache & Queue - Enhanced for extraction caching
  redis:
    image: redis:7-alpine
    container_name: chrono_redis
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    networks:
      - chrono_network
    restart: unless-stopped
    # Enhanced resource allocation for content caching
    deploy:
      resources:
        limits:
          memory: 1.5G  # Increased from 512M (for extraction caching)
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Meilisearch - Enhanced for content indexing
  meilisearch:
    image: getmeili/meilisearch:v1.16
    container_name: chrono_meilisearch
    volumes:
      - meilisearch_data:/meili_data
    environment:
      - MEILI_MASTER_KEY=RuvEMt9LztgYqdfqRFmZbT52uysNrt73ps57RZ2PRd53kjWxe2qiv9kadk9EiV5k
      - MEILI_ENV=development
      - MEILI_NO_ANALYTICS=true
      # Meilisearch performance optimization
      - MEILI_MAX_INDEXING_MEMORY=1gb
      - MEILI_MAX_INDEXING_THREADS=4
      - MEILI_LOG_LEVEL=INFO
    ports:
      - "7700:7700"
    networks:
      - chrono_network
    restart: unless-stopped
    # Enhanced resource allocation
    deploy:
      resources:
        limits:
          memory: 2G  # Increased from 1G (using freed Firecrawl memory)
          cpus: '2.0'
        reservations:
          memory: 1.5G
          cpus: '1.0'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7700/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Celery Worker - Optimized for robust extraction
  celery_worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_celery_worker
    command: celery -A app.tasks.celery_app worker --loglevel=info --queues=scraping,indexing,quick,celery,backup --concurrency=12 --max-tasks-per-child=50 --max-memory-per-child=800000
    volumes:
      - ./backend:/app:z
      - backup_data:/app/backups:z
      - backup_logs:/app/logs:z
    environment:
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      - REDIS_HOST=redis
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - ENVIRONMENT=development
      # OpenRouter configuration for background tasks
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - SMTP_HOST=mailpit
      - SMTP_PORT=1025
      - SMTP_TLS=false
      - EMAILS_FROM_EMAIL=noreply@chrono-scraper.local
      - EMAILS_FROM_NAME=Chrono Scraper Dev
      # Robust extraction configuration
      - USE_INTELLIGENT_EXTRACTION_ONLY=true
      - INTELLIGENT_EXTRACTION_CONCURRENCY=15
      - ROBUST_EXTRACTION_ENABLED=true
      - EXTRACTION_TIMEOUT=45
      - EXTRACTION_CACHE_TTL=3600
      # Individual content extraction enabled
      - USE_INDIVIDUAL_EXTRACTION=true
      # Proxy Configuration for Archive.org access
      - USE_PROXY=${USE_PROXY:-false}
      - PROXY_SERVER=${PROXY_SERVER:-}
      - PROXY_USERNAME=${PROXY_USERNAME:-}
      - PROXY_PASSWORD=${PROXY_PASSWORD:-}
      # Backup System Configuration
      - BACKUP_ENABLED=${BACKUP_ENABLED:-true}
      - BACKUP_LOCAL_ENABLED=${BACKUP_LOCAL_ENABLED:-true}
      - BACKUP_LOCAL_PATH=${BACKUP_LOCAL_PATH:-/app/backups}
      - BACKUP_ENCRYPTION_KEY=${BACKUP_ENCRYPTION_KEY:-}
      - BACKUP_LOG_LEVEL=${BACKUP_LOG_LEVEL:-INFO}
    depends_on:
      - postgres
      - redis
    networks:
      - chrono_network
    restart: unless-stopped
    # Enhanced resource allocation for extraction processing
    deploy:
      resources:
        limits:
          memory: 4G  # Significantly increased (primary extraction worker)
          cpus: '3.0'
        reservations:
          memory: 2G
          cpus: '2.0'

  # Celery Beat Scheduler
  celery_beat:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_celery_beat
    command: celery -A app.tasks.celery_app beat --loglevel=info
    volumes:
      - ./backend:/app:z
    environment:
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=chrono_scraper
      - POSTGRES_PASSWORD=chrono_scraper_dev
      - POSTGRES_DB=chrono_scraper
      - REDIS_HOST=redis
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - ENVIRONMENT=development
      # OpenRouter configuration for scheduled tasks
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - SMTP_HOST=mailpit
      - SMTP_PORT=1025
      - SMTP_TLS=false
      - EMAILS_FROM_EMAIL=noreply@chrono-scraper.local
      - EMAILS_FROM_NAME=Chrono Scraper Dev
      # Robust extraction configuration
      - USE_INTELLIGENT_EXTRACTION_ONLY=true
      - INTELLIGENT_EXTRACTION_CONCURRENCY=15
      - ROBUST_EXTRACTION_ENABLED=true
    depends_on:
      - postgres
      - redis
    networks:
      - chrono_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Flower - Celery Monitoring
  flower:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_flower
    command: celery -A app.tasks.celery_app flower --port=5555
    volumes:
      - ./backend:/app:z
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      - redis
    networks:
      - chrono_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'

  # Mailpit - Email Testing
  mailpit:
    image: axllent/mailpit:latest
    container_name: chrono_mailpit
    ports:
      - "1025:1025"
      - "8025:8025"
    networks:
      - chrono_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.125'

  # Robust Extraction Metrics Service (Optional)
  extraction_monitor:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: chrono_extraction_monitor
    command: python -m app.scripts.extraction_monitor
    volumes:
      - ./backend:/app:z
    environment:
      - REDIS_HOST=redis
      - BACKEND_URL=http://backend:8000
      - MONITOR_INTERVAL=60
      - LOG_LEVEL=INFO
    depends_on:
      - redis
      - backend
    networks:
      - chrono_network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.125'

networks:
  chrono_network:
    driver: bridge